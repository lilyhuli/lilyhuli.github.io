<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TangDon的博客</title>
  
  <subtitle>我叫Tangdong，一位目前在北京工作的软件研发工程师，这个lilyhuli.cn域名买了很久了，作为github.io的静态博客的主要域名，博客主要记录学习的笔记和杂文，更新比较慢。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="lilyhuli.github.io/"/>
  <updated>2019-10-30T16:46:09.000Z</updated>
  <id>lilyhuli.github.io/</id>
  
  <author>
    <name>TangDon</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark Sql 篇1</title>
    <link href="lilyhuli.github.io/2019/10/30/hello/"/>
    <id>lilyhuli.github.io/2019/10/30/hello/</id>
    <published>2019-10-30T15:26:27.000Z</published>
    <updated>2019-10-30T16:46:09.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-SparkSQL-概述"><a href="#1-SparkSQL-概述" class="headerlink" title="1 SparkSQL 概述"></a>1 SparkSQL 概述</h3><p>本章节测试代码已提交至：<a href="https://github.com/lilyhuli/spark" target="_blank" rel="noopener">https://github.com/lilyhuli/spark</a></p><h4 id="1-1-什么是Spark-SQL"><a href="#1-1-什么是Spark-SQL" class="headerlink" title="1.1    什么是Spark SQL"></a>1.1    什么是Spark SQL</h4><p>&ensp;&ensp;&ensp;&ensp;Spark SQL 是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象 DataFrame 和 DataSet，并且作为分布式SQL查询引擎的作用。</p><h4 id="1-2-Spark-SQL的特点"><a href="#1-2-Spark-SQL的特点" class="headerlink" title="1.2    Spark SQL的特点"></a>1.2    Spark SQL的特点</h4><p>1易整合  2统一的数据访问方式  3兼容Hive  4标准的数据连接</p><h4 id="1-3-什么是DataFrame"><a href="#1-3-什么是DataFrame" class="headerlink" title="1.3    什么是DataFrame"></a>1.3    什么是DataFrame</h4><p>&ensp;&ensp;&ensp;&ensp;与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。</p><p><img src="https://github.com/lilyhuli/spark/raw/master/file/rddAndDataFrame.png" alt="avatar"></p><p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。性能上比RDD要高，主要原因：<br>优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</p><p><img src="https://github.com/lilyhuli/spark/raw/master/file/catalyst.png" alt="avatar"></p><p><img src="https://github.com/lilyhuli/spark/raw/master/file/eg.png" alt="avatar"></p><p><img src="https://github.com/lilyhuli/spark/raw/master/file/eg1.png" alt="avatar"></p><p> 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><h4 id="1-4-什么是DataSet"><a href="#1-4-什么是DataSet" class="headerlink" title="1.4    什么是DataSet"></a>1.4    什么是DataSet</h4><p>1）是Dataframe API的一个扩展，是Spark最新的数据抽象。<br>2）用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。<br>3）Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。<br>4）样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。<br>5） Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。    Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。<br>6）DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].<br>7）DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-SparkSQL-概述&quot;&gt;&lt;a href=&quot;#1-SparkSQL-概述&quot; class=&quot;headerlink&quot; title=&quot;1 SparkSQL 概述&quot;&gt;&lt;/a&gt;1 SparkSQL 概述&lt;/h3&gt;&lt;p&gt;本章节测试代码已提交至：&lt;a href=&quot;https
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>HashMap 与 concurrentHashMap</title>
    <link href="lilyhuli.github.io/2019/03/12/HashMap-%E4%B8%8E-concurrentHashMap/"/>
    <id>lilyhuli.github.io/2019/03/12/HashMap-与-concurrentHashMap/</id>
    <published>2019-03-12T13:49:32.000Z</published>
    <updated>2019-03-12T14:21:04.742Z</updated>
    
    <content type="html"><![CDATA[<p> HashMap<br> 1.7的HashMap<br> <img src="https://javadoop.com/blogimages/map/1.png" alt="Image text"></p><p> <code>static Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;{     final K key;   V value;   Entry&lt;K,V&gt; next;   int hash; }</code></p><p> HashMap 里是一个数组,然后数组中每一个元素是一个单向链表。<br> 每一个绿色的实体是嵌套类Entry实体 Entry包括 key value hash和用于单向链表的next</p><p> capacity 当前数组的容量 始终保持2^n 可以扩容 扩容后数组大小是当前的2倍</p><p> loadFactor 负载银子 默认为0.75</p><p> threshold：扩容的阈值，等于 capacity * loadFactor</p><p>因为要参与与操作 所以要把数组的大小保持为2的n次方的做法，Java7和Java的HashMap 和 ConcurrentHashMap都有相应要求，只不过实现的代码稍微有些不同</p><p> 由于是双倍扩容 迁移过程中 会将原来的table[i] 中链表的所有节点 分拆到新的数组 newTable[i]和newTable[i+oldLength]位置上。如原来的数组16 那么0索引的数据会被重新安排到0 和16上</p><p> 1.7的ConcurrentHashMap<br> 整个ConcurrentHashMap由一个个Segment组成 Segment代表段的意思<br>，很多地方都叫分段锁，每次加锁都锁住segment 锁的粒度很小，只要保证了每个Segment是线程安全的 也就实现了全局的线程安全 Concurrent默认是16 所以说ConcurrentHashMap 有 16个Segments 最多支持16个线程并发写，还有需要注意的是一旦值初始化是不可以扩容的；</p><p>1.8 HashMap<br><img src="https://javadoop.com/blogimages/map/2.png" alt="Image text"></p><p>和7最大的不同就是利用了红黑树，所以其由 数组+链表+红黑树组成<br>1.7的查找速时间长度取决于链表的长度 O(n) 采用了红黑树之后，当链表元素超过8个自动转成红黑树，时间复杂度变成了O（logn）；</p><p>1.8的ConcurrentHashMap也采用了红黑树</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; HashMap&lt;br&gt; 1.7的HashMap&lt;br&gt; &lt;img src=&quot;https://javadoop.com/blogimages/map/1.png&quot; alt=&quot;Image text&quot;&gt;&lt;/p&gt;
&lt;p&gt; &lt;code&gt;static Entry&amp;lt;K,V&amp;gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>死锁</title>
    <link href="lilyhuli.github.io/2019/03/12/%E6%AD%BB%E9%94%81/"/>
    <id>lilyhuli.github.io/2019/03/12/死锁/</id>
    <published>2019-03-12T12:58:28.000Z</published>
    <updated>2019-03-12T13:05:31.875Z</updated>
    
    <content type="html"><![CDATA[<p>死锁发生的必要条件<br>1：互斥条件<br>2：请求和保持条件<br>3：不剥夺条件<br>4：环路等待条件</p><p>o1sy时拿住o2<br>o2sy时拿住o1</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;死锁发生的必要条件&lt;br&gt;1：互斥条件&lt;br&gt;2：请求和保持条件&lt;br&gt;3：不剥夺条件&lt;br&gt;4：环路等待条件&lt;/p&gt;
&lt;p&gt;o1sy时拿住o2&lt;br&gt;o2sy时拿住o1&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Java 之 JUC</title>
    <link href="lilyhuli.github.io/2019/03/12/Untitled/"/>
    <id>lilyhuli.github.io/2019/03/12/Untitled/</id>
    <published>2019-03-12T09:28:56.000Z</published>
    <updated>2019-03-12T12:56:59.305Z</updated>
    
    <content type="html"><![CDATA[<p> java.util.concurrent</p><p> volatile  当多个线程进行操作共享数据时,可以保证内存中的数据是可见的;相较于 synchronized 是一种<br>较为轻量级的同步策略;<br>volatile 不具备”互斥性”;<br>volatile 不能保证变量的”原子性”;</p><p>原子性<br>atomic 提供了常用的原子变量 使用volatile修饰 保证了内存可见性<br>.getAndIncrement() 自增运算</p><p>i++的操作实际上分为三个步骤: “读-改-写”;</p><p>CAS算法是硬件对于并发的支持，针对多处理器操作而设计的处理器中的一种特殊指令，用于管理对共享数据的并发访问；</p><p>CAS是一种无锁的非阻塞算法的实现</p><p>ConcurrentHashMap 同步容器类是java5增加的一个线程安全的哈希表 介于hashmap与hashtable之间 内部采用锁分段机制替代hashtable的独占锁 进而提高性能  关键点 分段锁 替代 独占锁</p><p>此包还提供了设计用于多线程上下文中的Collection实现: ConcurrentHashMap,ConcurrentSkipListMap<br>ConcurrentSkipListSet, CopyOnWriteArrayList 和 CopyOnWriteArraySet;</p><p>当期望许多线程访问一个给定collection时,ConcurrentHashMap通常优于同步的HashMap;<br>ConcurrentSkipListMap通常优于同步的TreeMap;<br>当期望的读数和遍历远远大于列表的更新数时, CopyOnWriteArrayList优于同步的ArrayList;</p><p>CountDownLatch是一个同步辅助类,在完成一组正在其他线程中执行的操作之前,它允许一个或多个线程一直等待;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; java.util.concurrent&lt;/p&gt;
&lt;p&gt; volatile  当多个线程进行操作共享数据时,可以保证内存中的数据是可见的;相较于 synchronized 是一种&lt;br&gt;较为轻量级的同步策略;&lt;br&gt;volatile 不具备”互斥性”;&lt;br&gt;volati
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>redis 5种数据类型</title>
    <link href="lilyhuli.github.io/2019/03/12/redis-%E7%89%A9%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>lilyhuli.github.io/2019/03/12/redis-物种数据类型/</id>
    <published>2019-03-11T17:08:52.000Z</published>
    <updated>2019-03-11T17:10:22.607Z</updated>
    
    <content type="html"><![CDATA[<p>RedisTemplate基本操作</p><p>Spring Data Redis 提供了一个工具类：RedisTemplate。里面封装了对于Redis的五种数据结构的各种操作，包括：</p><ul><li>redisTemplate.opsForValue() ：操作字符串</li><li>redisTemplate.opsForHash() ：操作hash</li><li>redisTemplate.opsForList()：操作list</li><li>redisTemplate.opsForSet()：操作set</li><li>redisTemplate.opsForZSet()：操作zset</li></ul><p>其它一些通用命令，如expire，可以通过redisTemplate.xx()来直接调用</p><p>5种结构：</p><ul><li>String：等同于java中的，<code>Map&lt;String,String&gt;</code></li><li>list：等同于java中的<code>Map&lt;String,List&lt;String&gt;&gt;</code></li><li>set：等同于java中的<code>Map&lt;String,Set&lt;String&gt;&gt;</code></li><li>sort_set：可排序的set</li><li>hash：等同于java中的：<code>Map&lt;String,Map&lt;String,String&gt;&gt;</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;RedisTemplate基本操作&lt;/p&gt;
&lt;p&gt;Spring Data Redis 提供了一个工具类：RedisTemplate。里面封装了对于Redis的五种数据结构的各种操作，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;redisTemplate.opsForValue() ：
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>静态化</title>
    <link href="lilyhuli.github.io/2019/03/12/%E9%9D%99%E6%80%81%E5%8C%96/"/>
    <id>lilyhuli.github.io/2019/03/12/静态化/</id>
    <published>2019-03-11T17:00:49.000Z</published>
    <updated>2019-03-11T17:08:34.504Z</updated>
    
    <content type="html"><![CDATA[<p>静态化是指把动态生成的HTML页面变为静态内容保存，以后用户的请求到来，直接访问静态页面，不再经过服务的渲染。</p><p>而静态的HTML页面可以部署在nginx中，从而大大提高并发能力，减小tomcat压力。</p><p>目前，静态化页面都是通过模板引擎来生成，而后保存到nginx服务器来部署。常用的模板引擎比如：</p><ul><li>Freemarker</li><li>Velocity</li><li>Thymeleaf</li></ul><p>Thymeleaf除了可以把渲染结果写入Response，也可以写到本地文件，从而实现静态化。</p><p>接下来，我们修改nginx，让它对商品请求进行监听，指向本地静态页面，如果本地没找到，才进行反向代理：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;静态化是指把动态生成的HTML页面变为静态内容保存，以后用户的请求到来，直接访问静态页面，不再经过服务的渲染。&lt;/p&gt;
&lt;p&gt;而静态的HTML页面可以部署在nginx中，从而大大提高并发能力，减小tomcat压力。&lt;/p&gt;
&lt;p&gt;目前，静态化页面都是通过模板引擎来生成，而后
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SPU和SKU</title>
    <link href="lilyhuli.github.io/2019/03/12/SPU%E5%92%8CSKU/"/>
    <id>lilyhuli.github.io/2019/03/12/SPU和SKU/</id>
    <published>2019-03-11T16:57:57.000Z</published>
    <updated>2019-03-11T17:00:33.455Z</updated>
    
    <content type="html"><![CDATA[<p>按照这个理解，SPU就是俗称的“款”；SKU就是商品的“件”。</p><p>spu 和 sku 都是属性值的集合, 举个栗子 &gt;&gt;一部 6S, 它身上有很多的属性和值. 比如 :毛重: 420.00 g产地: 中国大陆容量: 16G, 64G, 128G颜色: 银, 白, 玫瑰金你跑进苏宁顺电, 说想要一台 6S, 店员也会再继续问: 你想要什么 6S? 16G 银色? 64G 白色?每一台 6S 的毛重都是 420.00 g, 产地也都是 中国大陆. 这两个属性就属于 spu 属性.而容量和颜色, 这种会影响价格和库存的(比如 16G 与 64G 的价格不同, 16G 银色还有货, 金色卖完了)属性就是 sku 属性.spu 属性(不会影响到库存和价格的属性, 又叫关键属性) &gt;&gt;毛重: 420.00 g产地: 中国大陆sku 属性(会影响到库存和价格的属性, 又叫销售属性) &gt;&gt;容量: 16G, 64G, 128G颜色: 银, 白, 玫瑰金sku 在生成时, 会根据 属性生成 相应的 笛卡尔积.想像一下扑克牌的黑红梅方和 A-K, 扑克牌是这样的 sku 属性:牌面: A - K花色: 黑红梅方最终会生成 13 <em> 4 = 52 张牌, 上面的 6S 则会生成 3 </em> 3 = 9 个 SKU商品 : iphone 6sspu : 包含在每一部 6s 的属性集合, 与商品是一对一的关系(产地:中国, 毛重:420g…)sku : 影响价格和库存的 属性集合, 与商品是多对一的关系单品 : 同 sku. 国人的另一种叫法!~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;按照这个理解，SPU就是俗称的“款”；SKU就是商品的“件”。&lt;/p&gt;
&lt;p&gt;spu 和 sku 都是属性值的集合, 举个栗子 &amp;gt;&amp;gt;一部 6S, 它身上有很多的属性和值. 比如 :毛重: 420.00 g产地: 中国大陆容量: 16G, 64G, 128G颜色:
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用spring ResponseEntity处理http响应</title>
    <link href="lilyhuli.github.io/2019/03/12/%E4%BD%BF%E7%94%A8spring-ResponseEntity%E5%A4%84%E7%90%86http%E5%93%8D%E5%BA%94/"/>
    <id>lilyhuli.github.io/2019/03/12/使用spring-ResponseEntity处理http响应/</id>
    <published>2019-03-11T16:54:34.000Z</published>
    <updated>2019-03-11T16:56:15.494Z</updated>
    
    <content type="html"><![CDATA[<p>使用spring时，达到同一目的通常有很多方法，对处理http响应也是一样。本文我们学习如何通过ResponseEntity设置http相应内容、状态以及头信息。ResponseEntity<br>ResponseEntity标识整个http相应：状态码、头部信息以及相应体内容。因此我们可以使用其对http响应实现完整配置。</p><p>如果需要使用ResponseEntity，必须在请求点返回，通常在spring rest中实现。ResponseEntity是通用类型，因此可以使用任意类型作为响应体：<br>@GetMapping(“/hello”)<br>ResponseEntity<string> hello() {<br>    return new ResponseEntity&lt;&gt;(“Hello World!”, HttpStatus.OK);</string></p><p>  简单粗暴的讲</p><p>@ResponseBody可以直接返回Json结果，</p><p>@ResponseEntity不仅可以返回json结果，还可以定义返回的HttpHeaders和HttpStatus<br>}</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用spring时，达到同一目的通常有很多方法，对处理http响应也是一样。本文我们学习如何通过ResponseEntity设置http相应内容、状态以及头信息。ResponseEntity&lt;br&gt;ResponseEntity标识整个http相应：状态码、头部信息以及相应体
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ES 搜索</title>
    <link href="lilyhuli.github.io/2019/03/12/ES-%E6%90%9C%E7%B4%A2/"/>
    <id>lilyhuli.github.io/2019/03/12/ES-搜索/</id>
    <published>2019-03-11T16:18:32.000Z</published>
    <updated>2019-03-11T16:40:19.116Z</updated>
    
    <content type="html"><![CDATA[<p>es与kibana结合 在kibana中编写restful命令</p><p>索引（indices）——————————–Databases 数据库</p><p>​    类型（type）—————————–Table 数据表</p><p>​         文档（Document）—————-Row 行</p><p>​           字段（Field）——————-Columns 列 </p><p>映射是定义文档的过程，文档包含哪些字段，这些字段是否保存，是否索引，是否分词等</p><p>只有配置清楚，Elasticsearch才会帮我们进行索引库的创建（不一定）</p><ul><li><p>String类型，又分两种：</p><ul><li>text：可分词，不可参与聚合</li><li>keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合</li></ul></li><li><p>Numerical：数值类型，分两类</p><ul><li>基本数据类型：long、interger、short、byte、double、float、half_float</li><li>浮点数的高精度类型：scaled_float<ul><li>需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再还原。</li></ul></li></ul></li><li><p>Date：日期类型</p><p>elasticsearch可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为long，节省空间。</p><p>@Document(indexName = “item”,type = “docs”, shards = 1, replicas = 0)<br>public class Item {<br>  @Id<br>  private Long id;</p><p>  @Field(type = FieldType.Text, analyzer = “ik_max_word”)<br>  private String title; //标题</p><p>  @Field(type = FieldType.Keyword)<br>  private String category;// 分类</p><p>  @Field(type = FieldType.Keyword)<br>  private String brand; // 品牌</p><p>  @Field(type = FieldType.Double)<br>  private Double price; // 价格</p><p>  @Field(index = false, type = FieldType.Keyword)<br>  private String images; // 图片地址<br>}</p></li></ul><p>  Spring Data通过注解来声明字段的映射属性，有下面的三个注解：</p><ul><li><code>@Document</code> 作用在类，标记实体类为文档对象，一般有两个属性<ul><li>indexName：对应索引库名称</li><li>type：对应在索引库中的类型</li><li>shards：分片数量，默认5</li><li>replicas：副本数量，默认1</li></ul></li><li><code>@Id</code> 作用在成员变量，标记一个字段作为id主键</li><li><code>@Field</code> 作用在成员变量，标记为文档的字段，并指定字段映射属性：<ul><li>type：字段类型，取值是枚举：FieldType</li><li>index：是否索引，布尔类型，默认是true</li><li>store：是否存储，布尔类型，默认是false</li><li>analyzer：分词器名称</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;es与kibana结合 在kibana中编写restful命令&lt;/p&gt;
&lt;p&gt;索引（indices）——————————–Databases 数据库&lt;/p&gt;
&lt;p&gt;​    类型（type）—————————–Table 数据表&lt;/p&gt;
&lt;p&gt;​         文档（Do
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>分布式锁</title>
    <link href="lilyhuli.github.io/2019/02/28/zookeeper%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
    <id>lilyhuli.github.io/2019/02/28/zookeeper的应用场景/</id>
    <published>2019-02-28T12:51:23.000Z</published>
    <updated>2019-03-11T17:08:38.571Z</updated>
    
    <content type="html"><![CDATA[<p>redis的redlock，是redis官方支持的一种分布式锁算法，<br>给redis</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;redis的redlock，是redis官方支持的一种分布式锁算法，&lt;br&gt;给redis&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>随记</title>
    <link href="lilyhuli.github.io/2019/02/26/%E9%9A%8F%E8%AE%B0/"/>
    <id>lilyhuli.github.io/2019/02/26/随记/</id>
    <published>2019-02-25T16:53:57.000Z</published>
    <updated>2019-03-11T16:40:43.581Z</updated>
    
    <content type="html"><![CDATA[<p>Rabbitmq：假设P1和C1注册了相同的Broker，Exchange和Queue。P1发送的消息最终会被C1消费。基本的通信流程大概如下所示：</p><p>P1生产消息，发送给服务器端的Exchange<br>Exchange收到消息，根据ROUTINKEY，将消息转发给匹配的Queue1<br>Queue1收到消息，将消息发送给订阅者C1<br>C1收到消息，发送ACK给队列确认收到消息<br>Queue1收到ACK，删除队列中缓存的此条消息</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Rabbitmq：假设P1和C1注册了相同的Broker，Exchange和Queue。P1发送的消息最终会被C1消费。基本的通信流程大概如下所示：&lt;/p&gt;
&lt;p&gt;P1生产消息，发送给服务器端的Exchange&lt;br&gt;Exchange收到消息，根据ROUTINKEY，将消息
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MQ面试连环炮</title>
    <link href="lilyhuli.github.io/2019/02/24/MQ%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/"/>
    <id>lilyhuli.github.io/2019/02/24/MQ面试连环炮/</id>
    <published>2019-02-24T10:27:56.000Z</published>
    <updated>2019-02-25T17:51:37.525Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4" face="黑体">1、消息队列的作用？</font><br><br></p><p><font size="2">核心3点：解耦、异步、削峰<br>1.abcd系统问题 2.大量用户通过浏览器（举办了一场活动）在中午高峰期进行了大量操作，大量系统涌入系统，比如说5000/秒,系统基于mysql，大量请求涌入mysql，每秒对mysql5000次冲击基本上挂掉，因为一般的mysql2000请求就会挂了，但是高峰期过了之后到下午就是低峰期，为了保证mysql不被打死，我们采用的方案就是加入一个mq，在请求端加入mq，系统a从mq中慢慢拉取请求，过了高峰期挤压的消息，就算不加机器一个小时也能消耗完。</font><br><br></p><p><font size="4" face="黑体">2、保证消息队列的高可用？</font><br><br></p><p><font size="2">MQ的缺点很明确，加入mq会让系统变得复杂，rabbitmq的高可用方式：rabbitmq有三种模式：单机模式，普通集群模式，镜像集群模式   1.单机模式没人用，2普通集群模式，就是在多台机器上启动多台实例，每个机器启动一个。但是你创建的queue只会在一个rabbitmq上，每个实例都会同步queue的元数据。Channel 声明队列 3.集群镜像模式 把需要的队列做成镜像队列，存在于多个节点，属于rabbitmq的ha方案。其实质和普通模式不同之处在于，消息实体会主动在镜像节点间同步，而不是在consumer取数据时临时拉取。kafka的高可用性：多个broker组成，每个broker是一个节点；你创建一个topic，这个topic可以划分为多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据。kafka 0.8以后，提供了HA机制，就是replica副本机制。每个partition的数据都会同步到吉他机器上，形成自己的多个replica副本。然后所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower。</font><br><br></p><p><font size="4" face="黑体">2、保证消息队列的幂等性？</font><br><br></p><p><font size="2">业界主流的幂等性有两种操作：</font></p><p>1.唯一 ID + 指纹码 机制，利用数据库主键去重</p><p>2.利用redis的原子性去实现</p><p>kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交一下，代表我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的offset来继续消费吧。</p><p>但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接kill进程了，再重启。这会导致consumer有些消息处理了，但是没来得及提交offset，尴尬了。重启之后，少数消息会再次消费一次。<br>给你举个例子吧。假设你有个系统，消费一条往数据库里插入一条，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下已经消费过了，直接扔了，不就保留了一条数据？</p><p>一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性</p><p>幂等性，我通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。<br>其实还是得结合业务来思考，我这里给几个思路：<br>（1）比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update一下好吧</p><p>（2）比如你是写redis，那没问题了，反正每次都是set，天然幂等性</p><p>（3）比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后你这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个id写redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。<br><br></p><p><font size="4" face="黑体">3、保证消息的可靠性传输（如何处理消息丢失的问题）？</font><br></p><p><font size="2">rabbitmq 1生产者弄丢了数据<br>confirm机制 2 rabbitmq弄丢了数据,设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。3 消费端弄丢了数据 这个时候得用rabbitmq提供的ack机制，简单来说，就是你关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次你自己代码里确保处理完的时候，再程序里ack一把。这样的话，如果你还没处理完，不就没有ack？那rabbitmq就认为你还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。kafka  kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 kafka弄丢了数据<br>给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本</font></p><p>在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧</p><p>在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了</p><p>在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了</p><p>我们生产环境就是按照上述要求配置的，这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失<br><br><br></p><p><font size="4" face="黑体">4、保证消息的顺序性？</font><br><br></p><p><font size="2">那如何保证消息的顺序性呢？简单简单</font></p><p>（1）rabbitmq：拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理</p><p>（2）kafka：一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可<br><br><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;4&quot; face=&quot;黑体&quot;&gt;1、消息队列的作用？&lt;/font&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;2&quot;&gt;核心3点：解耦、异步、削峰&lt;br&gt;1.abcd系统问题 2.大量用户通过浏览器（举办了一场活动）在中午高峰期进行了大量操作，大量
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spring 面试题总结</title>
    <link href="lilyhuli.github.io/2019/02/16/myNewTestBlog/"/>
    <id>lilyhuli.github.io/2019/02/16/myNewTestBlog/</id>
    <published>2019-02-15T18:56:43.000Z</published>
    <updated>2019-02-22T13:34:03.825Z</updated>
    
    <content type="html"><![CDATA[<font size="4" face="黑体">1、什么是 Spring 框架？Spring 框架有哪些主要模块？</font><br><br><br><font size="2">Spring 框架是一个为 Java 应用程序的开发提供了综合、广泛的基础性支持的 Java 平台。Spring帮助开发者解决了开发中基础性的问题，使得开发人员可以专注于应用程序的开发。Spring 框架本身亦是按照设计模式精心打造，这使得我们可以在开发环境中安心的集成 Spring 框架，不必担心 Spring 是如何在后台进行工作的。Spring 框架至今已集成了 20 多个模块。这些模块主要被分如下图所示的核心容器、数据访问/集成,、Web、AOP（面向切面编程）、工具、消息和测试模块。</font><br><br><br><font size="4" face="黑体">2、Spring 框架中都用到了哪些设计模式？</font><br><br><br><font size="2">Spring 框架中使用到了大量的设计模式，下面列举了比较有代表性的：1、代理模式—在 AOP 和 remoting 中被用的比较多。2、单例模式：在 spring 配置文件中定义的 bean 默认为单例模式。3、模板模式：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。4、委派模式：Spring 提供了 DispatcherServlet 来对请求进行分发。5、工厂模式：BeanFactory 用来创建对象的实例，贯穿于 BeanFactory / ApplicationContext接口的核心理念。6、代理模式：AOP 思想的底层实现技术，Spring 中采用 JDK Proxy 和 CgLib 类库。</font>]]></content>
    
    <summary type="html">
    
      
      
        &lt;font size=&quot;4&quot; face=&quot;黑体&quot;&gt;1、什么是 Spring 框架？Spring 框架有哪些主要模块？&lt;/font&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;font size=&quot;2&quot;&gt;Spring 框架是一个为 Java 应用程序的开发提供了综合、广泛的基础性支持的 Java 平
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>秒杀，了解一下</title>
    <link href="lilyhuli.github.io/2019/01/18/%E7%A7%92%E6%9D%80%EF%BC%8C%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B/"/>
    <id>lilyhuli.github.io/2019/01/18/秒杀，了解一下/</id>
    <published>2019-01-18T14:33:45.000Z</published>
    <updated>2019-01-18T14:35:57.058Z</updated>
    
    <content type="html"><![CDATA[<h1 id="秒杀"><a href="#秒杀" class="headerlink" title="秒杀"></a>秒杀</h1><p>一个关于秒杀的故事 记录学习和工作点滴</p><p>学习了redis和rabbitmq一直没有很好地应用，秒杀业务可以使用redis预减库存、消息队列异步下单逐级消减对数据库的访问；mysql最大的并发量才1000，</p><p>所以瓶颈基本都是在数据库</p><p>因为是前端菜鸟所以使用bootstrap jq 和后端的thymeleaf 中间件用rabbitmq/redis/druid；</p><p>系统支持横向拓展，集群，优化缓存 异步下单 消峰 验证码框架也是用的数学验证最大强度的防止机器人；</p><p>自己封装的jedis更易使用 使用jedisTemplate主要是spring给了cache功能，这里直接自己封装的。</p><p>未完待续。</p><p>项目地址：<a href="https://github.com/lilyhuli/miaosha" target="_blank" rel="noopener">https://github.com/lilyhuli/miaosha</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;秒杀&quot;&gt;&lt;a href=&quot;#秒杀&quot; class=&quot;headerlink&quot; title=&quot;秒杀&quot;&gt;&lt;/a&gt;秒杀&lt;/h1&gt;&lt;p&gt;一个关于秒杀的故事 记录学习和工作点滴&lt;/p&gt;
&lt;p&gt;学习了redis和rabbitmq一直没有很好地应用，秒杀业务可以使用redis预减库
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>springsecurity整合jwt</title>
    <link href="lilyhuli.github.io/2019/01/14/springsecurity%E6%95%B4%E5%90%88jwt/"/>
    <id>lilyhuli.github.io/2019/01/14/springsecurity整合jwt/</id>
    <published>2019-01-14T14:17:54.000Z</published>
    <updated>2019-01-14T14:18:57.976Z</updated>
    
    <content type="html"><![CDATA[<h1 id="springsecurity-jwt"><a href="#springsecurity-jwt" class="headerlink" title="springsecurity_jwt"></a>springsecurity_jwt</h1><p>springsecurity结合jwt<br>今天写下Spring Security整合jwt的一个简单小Demo，目的是登录后实现返回token</p><p>明天整合通用树形显示和vue前端显示 现在只能通过postman简单测试</p><p>jwt存在争议，但是很容易上手，分布式场景下不能再用ck-session的组合了</p><p>经典的三表结构比昨天的demo更有工业价值，代码放在Github上了 <a href="https://github.com/lilyhuli/springsecurity_jwt/" target="_blank" rel="noopener">https://github.com/lilyhuli/springsecurity_jwt/</a> 欢迎大家star、issues</p><p>ps：郁闷的七牛云图床必须要绑定备案域名了，后来发现issues直接拖拽就能当做图床，还是不要滥用呀。</p><p><img src="https://user-images.githubusercontent.com/32732399/51117727-956ae280-1849-11e9-8693-eb8787d27935.jpg" alt="Image text"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;springsecurity-jwt&quot;&gt;&lt;a href=&quot;#springsecurity-jwt&quot; class=&quot;headerlink&quot; title=&quot;springsecurity_jwt&quot;&gt;&lt;/a&gt;springsecurity_jwt&lt;/h1&gt;&lt;p&gt;spring
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spring Security </title>
    <link href="lilyhuli.github.io/2019/01/14/Spring-Security/"/>
    <id>lilyhuli.github.io/2019/01/14/Spring-Security/</id>
    <published>2019-01-13T16:34:56.000Z</published>
    <updated>2019-01-13T16:44:31.591Z</updated>
    
    <content type="html"><![CDATA[<p>   趁着周末闲来无事学习了一下Spring Security，源码在 <a href></a>lilyhuli/spring_security_exam</p><p>   用到的技术：<br>   java<br>   springboot<br>   mybatis<br>   mysql<br>   后端框架thymeleaf</p><p>需求如下：<br>Security中可以在内存中建立用户，留后门设置后门账号，以及数据库的结合访问。<br>和shiro一样 不同用户拥有不同权限，访问不同网页。<br>如果用户未登录，则返回首页，如果用户已登录，但是没有权限则返回json，告知。</p><pre><code>网上的看了很多文章，Spring Security内置了三个基于投票，AccessDecisionManager实现类，它们分别是AffirmativeBased、ConsensusBased 和UnanimousBased。我觉得继承AccessDecisionManager，然后重写decide方法就可以。当然Spring Security的代码需要一点点debug才能了解其原理。搞完又过0点了，洗洗睡了，梦里啥都有。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;   趁着周末闲来无事学习了一下Spring Security，源码在 &lt;a href&gt;&lt;/a&gt;lilyhuli/spring_security_exam&lt;/p&gt;
&lt;p&gt;   用到的技术：&lt;br&gt;   java&lt;br&gt;   springboot&lt;br&gt;   mybatis&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>又是一年末尾</title>
    <link href="lilyhuli.github.io/2018/12/25/%E5%8F%88%E6%98%AF%E4%B8%80%E5%B9%B4%E6%9C%AB%E5%B0%BE/"/>
    <id>lilyhuli.github.io/2018/12/25/又是一年末尾/</id>
    <published>2018-12-24T16:31:00.000Z</published>
    <updated>2019-01-13T16:33:42.291Z</updated>
    
    <content type="html"><![CDATA[<p>这一年不仅没有完成爸妈所说的人生大事，还把工作搞得一团糟。读了几本书，看了几部剧，就想把感想和经历写下来，毕竟好多年都没写总结了。</p><p>  1，《简爱》 </p><pre><code>长大了再读《简爱》真觉得简爱是一个自怨自艾的玛丽苏，根本没有灰姑娘的容颜，也没有纯洁谦卑，外柔内刚的品质，有的只是孤傲清高，目中无人，乖戾而非可爱，敏感且自卑，自我为中心，童年的经历给了她太多不幸，当然也影响了她的决定，她本不该，出身无法决定，但能决定的一定要好好把握；而《呼啸山庄》那种坏就是要坏的彻底，悲剧就是要绑架你；</code></pre><p>  2，《东京女子图鉴》</p><pre><code>绫在干什么？同作为一个京漂，绫这么富有野心，怎么可能找到和她一样的人呢，她看不起自己的阶层，厌恶自己的出身，她并不喜欢自己，努力去除自己身上的土气，那个面对阶级凝固努力想要嫁入上层阶级的绫，真当有平配自己的伴侣在一起时，要么嫌过于平凡，要么觉得对方野心太重，要么觉得对方无聊，最后都不欢而散，她只喜欢自己高攀不起的港区男，同样港区男是不会选择她的。成熟后的人多可怕，没有感情波动，和《黑镜》里玩大数据匹配伴侣的那集很像，不过黑镜的结局都要好过图鉴了，大多数东京人在工作中挣扎着对感情的投入太有限了。不过这样才显得更真实，毕竟电视剧和小说放大了感情，让你觉得它很重要；作为职业白领，绫的结婚只是她的一个目标，日本压抑的社会关系真让人喘不过气，偶像剧是泛娱乐时代的毒品，从早到晚谈恋爱，贫女出门遇见高富帅。</code></pre><p>3， 极简生活</p><pre><code>加入极简生活小组之后，发现大多数人的极简主义就是扔扔扔，这是一种的变相的消费升级，不是极简主义。对于像我这么穷的人来说，极简主义是断舍离，减物欲。购买物品所带来的快感实在太短了，在精美的包装下它让人非常期待，一旦拆开，我就要担心它会不会又磕伤划痕，变旧过时，这样所带来的只能是重蹈覆辙，不断陷入消费主义的陷阱里，不能自拔，这就非常恐怖了；我不是一个批判家，也没有资格判断，社会的标尺太沉重，对错正邪都只是前人框定好的，没有机会去定义，没有勇气去改变，更不是为了安利极简主义的生活方式，生活方式是自己决定的，根本没有对错，只要能让自己内心充盈，关注更重要的事，不必为了琐碎浪费时间。</code></pre><p>4， 情感</p><pre><code>在我看来，爱情在只不过是一小段插曲，是日常生活中诸多事务中的一件小事，这些都是从我的斗争经历里得出的。喜欢你的人会把你的沉默寡言看成斯文乖巧，莽撞冒失也可以是可爱娇痴，甚至脾气暴躁都可以是豁达爽朗；不喜欢你的人则把关心当做打扰，沉默当成冷落，你精心准备的心思会被当做神经质。所以形容词最不靠谱，舔狗也不得house。就没有那种成熟平等的恋爱么，再炙热也不过火，再沉默也不觉得是冷漠。恐惧不期而遇的爱蔓延，那种没有回应的尴尬和苦涩。毛姆不就说： “女人可以原谅男人伤害他，但绝不能原谅男人为她做出牺牲” ，毛姆是一个仇女的人，但他这句话说的很对。《一千零一夜》里的王后会沉迷每天对她暴虐的人，但是却对爱她的国王熟视无睹；所以不要对一个不爱你的人好，只会让自己显得更廉价。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这一年不仅没有完成爸妈所说的人生大事，还把工作搞得一团糟。读了几本书，看了几部剧，就想把感想和经历写下来，毕竟好多年都没写总结了。&lt;/p&gt;
&lt;p&gt;  1，《简爱》 &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;长大了再读《简爱》真觉得简爱是一个自怨自艾的玛丽苏，根本没有灰姑娘的容颜，也没
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>RabbitMQ 相关</title>
    <link href="lilyhuli.github.io/2018/05/14/RabbitMQ-%E7%9B%B8%E5%85%B3-2/"/>
    <id>lilyhuli.github.io/2018/05/14/RabbitMQ-相关-2/</id>
    <published>2018-05-13T16:28:00.000Z</published>
    <updated>2019-01-13T16:29:50.559Z</updated>
    
    <content type="html"><![CDATA[<p>摘要： 使用RabbitMQ的消息队列，可以有效提高系统的峰值处理能力。</p><p>RabbitMQ是消息代理(Message Broker)，它支持多种异步消息处理方式，最常见的有：</p><p>Work Queue：将消息缓存到一个队列，默认情况下，多个worker按照Round Robin的方式处理队列中的消息。每个消息只会分配给单个worker。</p><p>Publish/Subscribe：每个订阅消息的消费者都会收到消息，因此每个消息通常会分配给多个worker，每个worker对消息进行不同的处理。</p><p>RabbitMQ还支持Routing、Topics、以及Remote procedure calls (RPC)等方式。</p><p>对于不同的消息处理方式，有一点是相同的，RabbitMQ是介于消息的生产者和消费者的中间节点，负责缓存和分发消息。RabbitMQ接收来自生产者的消息，缓存到内存中，按照不同的方式分发给消费者。RabbitMQ还可以将消息写入磁盘，保证持久化，这样即使RabbitMQ意外崩溃了，消息数据不至于完全丢失。</p><p>为什么使用RabbitMQ？</p><p>最简单的一点在于，它支持Work Queue等不同的消息处理方式，可以用于不同的业务场景。RabbitMQ的Work Queue，即消息队列。</p><p>使用消息队列，可以将不算紧急、但是非常消耗资源的计算任务，以消息的方式插入到RabbitMQ的队列中，然后使用多个处理模块处理这些消息。</p><p>这样做最大的好处在于：提高了系统峰值处理能力。因为，来不及处理的消息缓存在RabbitMQ中，避免了同时进行大量计算导致系统因超负荷运行而崩溃。而那些来不及处理的消息，会在峰值过去之后慢慢处理掉。</p><p>另一个好处在于解耦。消息的生产者只需要将消息发送给RabbitMQ，这些消息什么时候处理完，不会影响生产者的响应性能。</p><p>安装并运行RabbitMQ</p><p>使用Docker运行RabbitMQ非常简单，只需要执行一条简单的命令：</p><p>sudo docker run -d –name rabbitmq -h rabbitmq -p 5672:5672 -v /var/lib/rabbitmq:/var/lib/rabbitmq registry.docker-cn.com/library/rabbitmq:3.7</p><p>-d : 后台运行容器</p><p>–name rabbitmq : 将容器的名字设为rabbitmq</p><p>-h rabbitmq : 将容器的主机名设为rabbitmq，希望RabbitMQ消息数据持久化保存到本地磁盘是需要设置主机名，因为RabbitMQ保存数据的目录为主机名</p><p>-p 5672:5672 : 将容器的5672端口映射为本地主机的5672端口，这样可以通过本地的5672端口访问rabbitmq</p><p>-v /var/lib/rabbitmq:/var/lib/rabbitmq：将容器的/var/lib/rabbitmq目录映射为本地主机的/var/lib/rabbitmq目录，这样可以将RabbitMQ消息数据持久化保存到本地磁盘，即使RabbitMQ容器被删除，数据依然还在。</p><p>消费者的消息确认机制</p><p>rabbitmq怎么知道消息被接受了呢？</p><p>acknowledge 确认机制  用手动模式</p><p>work消息模型（任务模型）</p><p>队列里的消息越来越多，内存总有存满的一天消息会丢失，消息堆积。耗时较长的比如发短信，需要用mq。</p><p>让多个消费者绑定一个队列，共同消费队列中的消息。队列中的消息一旦消费，就会消失，因此任务是不会被重复执行的。</p><p>channel.basicQos(1); 设置每次消费一个信息，解决消息堆积问题。</p><p>2.3.订阅模型分类</p><p>1、1个生产者，多个消费者</p><p>2、每一个消费者都有自己的一个队列</p><p>3、生产者没有将消息直接发送到队列，而是发送到了交换机</p><p>4、每个队列都要绑定到交换机</p><p>5、生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者获取的目的</p><p>X（Exchanges）：交换机一方面：接收生产者发送的消息。另一方面：知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。</p><p>Exchange类型有以下几种：</p><pre><code>Fanout：广播，将消息交给所有绑定到交换机的队列Direct：定向，把消息交给符合指定routing key 的队列Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列</code></pre><p>我们这里先学习</p><p>Fanout：即广播模式</p><p>Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！</p><p>订阅模型-Fanout</p><p>1） 可以有多个消费者</p><p>2）  每个消费者有自己的queue（队列）</p><p>3）  每个队列都要绑定到Exchange（交换机）</p><p>4）  生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定。</p><p>5）  交换机把消息发送给绑定过的所有队列</p><p>6）  队列的消费者都能拿到消息。实现一条消息被多个消费者消费</p><p>两个变化：</p><p>1）  声明Exchange，不再声明Queue</p><p>2）  发送消息到Exchange，不再发送到Queue</p><p>步骤：</p><p>// 获取到连接</p><p>// 获取通道</p><p>// 声明队列</p><p>// 绑定队列到交换机</p><p>// 定义队列的消费者</p><p>// 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用</p><p>// 监听队列，手动返回完成</p><p>订阅模型-Direct</p><p>有选择性的接收消息</p><p>在订阅模式中，生产者发布消息，所有消费者都可以获取所有消息。</p><p>在路由模式中，我们将添加一个功能 - 我们将只能订阅一部分消息。 例如，我们只能将重要的错误消息引导到日志文件（以节省磁盘空间），同时仍然能够在控制台上打印所有日志消息。</p><p>但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。</p><p>在Direct模型下，队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key）</p><p>消息的发送方在向Exchange发送消息时，也必须指定消息的routing key</p><p>绑定队列到交换机，同时指定需要订阅的routing key。需要 update、delete</p><p>订阅模型-Topic</p><p>Topic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！</p><p>Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert</p><p>通配符规则：</p><pre><code>#：匹配一个或多个词*：匹配不多不少恰好1个词</code></pre><p>举例：</p><pre><code>audit.#：能够匹配audit.irs.corporate 或者 audit.irsaudit.*：只能匹配audit.irs</code></pre><p>面试题：</p><p>如何避免消息丢失？</p><p>1）  消费者的ACK机制。可以防止消费者丢失消息。</p><p>2）  但是，如果在消费者消费之前，MQ就宕机了，消息就没了。</p><p>是可以将消息进行持久化呢？</p><p>要将消息持久化，前提是：队列、Exchange都持久化</p><p>durable 持久化</p><p>AmqpTemplate 统一的spring消息处理模板；</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘要： 使用RabbitMQ的消息队列，可以有效提高系统的峰值处理能力。&lt;/p&gt;
&lt;p&gt;RabbitMQ是消息代理(Message Broker)，它支持多种异步消息处理方式，最常见的有：&lt;/p&gt;
&lt;p&gt;Work Queue：将消息缓存到一个队列，默认情况下，多个worke
      
    
    </summary>
    
      <category term="编程人生" scheme="lilyhuli.github.io/categories/%E7%BC%96%E7%A8%8B%E4%BA%BA%E7%94%9F/"/>
    
    
      <category term="java" scheme="lilyhuli.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>丝袜哥</title>
    <link href="lilyhuli.github.io/2018/04/22/swagger/"/>
    <id>lilyhuli.github.io/2018/04/22/swagger/</id>
    <published>2018-04-22T05:40:00.000Z</published>
    <updated>2019-01-13T16:17:38.270Z</updated>
    
    <content type="html"><![CDATA[<p>1.2.Swagger-UI</p><p>丝袜哥</p><p>1.2.1.什么是OpenAPI</p><p>随着互联网技术的发展，现在的网站架构基本都由原来的后端渲染，变成了：前端渲染、前后端分离的形态，而且前端技术和后端技术在各自的道路上越走越远。  前端和后端的唯一联系，变成了API接口；API文档变成了前后端开发人员联系的纽带，变得越来越重要。</p><p>没有API文档工具之前，大家都是手写API文档的，在什么地方书写的都有，而且API文档没有统一规范和格式，每个公司都不一样。这无疑给开发带来了灾难。</p><p>OpenAPI规范（OpenAPI Specification 简称OAS）是Linux基金会的一个项目，试图通过定义一种用来描述API格式或API定义的语言，来规范RESTful服务开发过程。目前V3.0版本的OpenAPI规范已经发布并开源在github上 。</p><p>官网：<a href="https://github.com/OAI/OpenAPI-Specification" target="_blank" rel="noopener">https://github.com/OAI/OpenAPI-Specification</a></p><p>1.2.2.什么是swagger？</p><p>OpenAPI是一个编写API文档的规范，然而如果手动去编写OpenAPI规范的文档，是非常麻烦的。而Swagger就是一个实现了OpenAPI规范的工具集。</p><p>官网：<a href="https://swagger.io/" target="_blank" rel="noopener">https://swagger.io/</a></p><p>Swagger包含的工具集：</p><p>Swagger编辑器： Swagger Editor允许您在浏览器中编辑YAML中的OpenAPI规范并实时预览文档。</p><p>Swagger UI： Swagger UI是HTML，Javascript和CSS资产的集合，可以从符合OAS标准的API动态生成漂亮的文档。</p><p>Swagger Codegen：允许根据OpenAPI规范自动生成API客户端库（SDK生成），服务器存根和文档。</p><p>Swagger Parser：用于解析来自Java的OpenAPI定义的独立库</p><p>Swagger Core：与Java相关的库，用于创建，使用和使用OpenAPI定义</p><p>Swagger Inspector（免费）： API测试工具，可让您验证您的API并从现有API生成OpenAPI定义</p><p>SwaggerHub（免费和商业）： API设计和文档，为使用OpenAPI的团队构建。</p><p>常用注解说明</p><p>/**</p><p>@Api：修饰整个类，描述Controller的作用</p><p>@ApiOperation：描述一个类的一个方法，或者说一个接口</p><p>@ApiParam：单个参数描述</p><p>@ApiModel：用对象来接收参数</p><p>@ApiProperty：用对象接收参数时，描述对象的一个字段</p><p>@ApiResponse：HTTP响应其中1个描述</p><p>@ApiResponses：HTTP响应整体描述</p><p>@ApiIgnore：使用该注解忽略这个API</p><p>@ApiError ：发生错误返回的信息</p><p>@ApiImplicitParam：一个请求参数</p><p>@ApiImplicitParams：多个请求参数</p><p>*/</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.2.Swagger-UI&lt;/p&gt;
&lt;p&gt;丝袜哥&lt;/p&gt;
&lt;p&gt;1.2.1.什么是OpenAPI&lt;/p&gt;
&lt;p&gt;随着互联网技术的发展，现在的网站架构基本都由原来的后端渲染，变成了：前端渲染、前后端分离的形态，而且前端技术和后端技术在各自的道路上越走越远。  前端和后端的唯
      
    
    </summary>
    
      <category term="编程人生" scheme="lilyhuli.github.io/categories/%E7%BC%96%E7%A8%8B%E4%BA%BA%E7%94%9F/"/>
    
    
      <category term="java" scheme="lilyhuli.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>hexo博客的诞生</title>
    <link href="lilyhuli.github.io/2017/11/10/hexo/"/>
    <id>lilyhuli.github.io/2017/11/10/hexo/</id>
    <published>2017-11-10T09:52:00.000Z</published>
    <updated>2018-01-07T17:35:48.249Z</updated>
    
    <content type="html"><![CDATA[<p>该博客托管在github pages上，所以免去了备案等手续，域名还没有申请，暂时先用github的<br>。</p>   <p>友：市面上已经有很多现成的博客平台，为什么还要自己搭建一个？<br><br>   我：想到搭建的目的，一来是自己的拖延症越来越严重，一点点小借口都能摧毁曾经燃起的雄雄壮志，做一个独立博客，记录生活点滴，技术感悟，望自己能成长的快些；二来作为个人营销的门面，喜欢卖弄和自我营销的我当然还建一个独立的博客，简单的博客没什么技术含量，所以才更注重内容，符合极简主义的定位。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该博客托管在github pages上，所以免去了备案等手续，域名还没有申请，暂时先用github的&lt;br&gt;。&lt;/p&gt;


   &lt;p&gt;友：市面上已经有很多现成的博客平台，为什么还要自己搭建一个？&lt;br&gt;&lt;br&gt;   我：想到搭建的目的，一来是自己的拖延症越来越严重，一点点
      
    
    </summary>
    
    
      <category term="hexo" scheme="lilyhuli.github.io/tags/hexo/"/>
    
  </entry>
  
</feed>
